{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1) Data Exploration\n",
        "Different ways to visualise and understand data.\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "-sRtLlIso0l_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## a) Sampling and Plotting\n",
        "In this section you will learn the basics of sampling and plotting using python libraries.\n",
        "- Generate 1000 samples from a normal distribution with a mean of 5 and standard deviation of 1 using NumPy. Visualize the distribution using a histogram in Matplotlib.\n"
      ],
      "metadata": {
        "id": "1Xsjv5G4qo-9"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "G3M6bRPdowil"
      },
      "outputs": [],
      "source": [
        "import numpy as np #This library helps us generate and process sample data.\n",
        "import matplotlib.pyplot as plt #This library is used to visualise data.\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "norm = np.random.normal(5,1,1000)\n",
        "print(norm)"
      ],
      "metadata": {
        "id": "biinNxCAqyIN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.hist(norm)\n",
        "plt.xlabel(\"Values\")\n",
        "plt.ylabel(\"Density\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "_8LQyjB0wXpg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Repeat for an exponential distribution with an average rate of 3 and visualise it as a density graph."
      ],
      "metadata": {
        "id": "OC4G7oTM3tA4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gam = np.random.gamma(shape=2, scale=4, size=1000)\n",
        "print(gam)"
      ],
      "metadata": {
        "id": "5qFhX3ElxGXo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "count, binedge = np.histogram(gam, bins=30)\n",
        "pdf = count/sum(count)\n",
        "print(count) #number of values in each bin\n",
        "print(binedge) #border values of bin\n",
        "print(pdf) #probability of a number being in a bin\n",
        "plt.plot(binedge[1:], pdf)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "-S3_cYC0y5zJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Practice Problems:\n",
        "- Generate 1000 samples of a Uniform Distribution between 0 and 10. Visualise using a histogram.\n"
      ],
      "metadata": {
        "id": "t8CoSyXr2YLo"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "BNP06-nU3x1t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Generate a beta distribution with shape parameters alpha=2 and beta=5. Plot the probability density function (PDF) and cumulative distribution function (CDF)."
      ],
      "metadata": {
        "id": "yBJwYxx93wwz"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "QqVyUQds3nr2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## b) Filtering & Plotting from a Data Frame\n",
        "In this section you will learn how to filter and plot data from a given dataset."
      ],
      "metadata": {
        "id": "k_tDv-eY1pSj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd #This library is used to read dataframes (spreadsheets)"
      ],
      "metadata": {
        "id": "DE7qa-OR69bt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Read and display the given dataset Star_Properties.csv."
      ],
      "metadata": {
        "id": "wxwZFQwU7rt3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sp = pd.read_csv(\"Star_Properties.csv\")\n",
        "sp"
      ],
      "metadata": {
        "id": "U0SflCir7LlF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Filter the data to display only stars of spectral class \"O\". Visualise the data as a histrogram sorted by Star Type."
      ],
      "metadata": {
        "id": "XSBl5akI77pA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "os = sp.loc[sp['Spectral Class'] == 'O']\n",
        "plt.hist(os['Star type'])\n",
        "plt.xlabel(\"Star Type\")\n",
        "plt.ylabel(\"Count\")\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "hmUuE3xy8hR7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Plot all the stars in a Temperature vs Luminosity scatter plot."
      ],
      "metadata": {
        "id": "VR4DXG_bDkMf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lum = sp['Luminosity(L/Lo)']\n",
        "temp = sp['Temperature (K)']\n",
        "\n",
        "plt.scatter(temp, lum, s=100, alpha=0.6, edgecolor='black', linewidth=1)\n",
        "#plt.yscale('log')\n",
        "plt.xlabel('Temperature (K)')\n",
        "plt.ylabel('Luminosity(L/Lo)')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yuRrkmC6D5er"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Practice Problems\n",
        "- Filter out stars of Color Red. Visualise this data as a histrogram sorted by temperature.\n"
      ],
      "metadata": {
        "id": "t3AmQIHZ_Fvc"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ggItaASpBywB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Plot the stars with a Temperature in (5000,30000) and Luminosity in (0,0.01) in a Temperature vs Luminosity scatter plot.                                          \n",
        "Hint: Luminosity is a logarithmic scale, adjust the limits accordingly."
      ],
      "metadata": {
        "id": "MStyJiWaEkeb"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qj7v0x1EFUTH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- (Hard) Bonus: The first histogram (Star Type vs Count) is not a good representation of the data. Can you find a way to present the data is an easier format?                                          \n",
        "Hint: You need to plot a different type of graph."
      ],
      "metadata": {
        "id": "qG4TDNyCBzJK"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IziMeKAo_fea"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2) Cleaning Data\n"
      ],
      "metadata": {
        "id": "NUbbtz7My7e9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In this section you will learn how to clean data before using it for ML applications."
      ],
      "metadata": {
        "id": "A9uM30sbDaH9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "TtDdZ88YPJhm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## a) Missing Values\n",
        "- Import the dataset diabetes.xlsx and display it.\n"
      ],
      "metadata": {
        "id": "GACjeydnNHrO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "db = pd.read_excel(\"diabetes.xlsx\")\n",
        "db"
      ],
      "metadata": {
        "id": "wsq_UowbPOvS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Display the columns with missing values."
      ],
      "metadata": {
        "id": "ZgE8D70B7yS5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Identify missing values\n",
        "for i in db.columns:\n",
        "  counts = db[i].value_counts()\n",
        "  if 0 in counts:  # Check if 0 is in the index\n",
        "    print(f\"Column '{i}' has {counts[0]} values at 0.\")"
      ],
      "metadata": {
        "id": "JJw3PXo77vo5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Deal with the missing values for\n",
        "\n",
        "i.  Blood Pressure, Insulin and BMI. (using imputation)\n",
        "\n",
        "ii. Skin Thickness and Glucose. (using forward filling)"
      ],
      "metadata": {
        "id": "Sbl3C1lkTaHZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#method 1: imputation\n",
        "for cl in ['BloodPressure', 'Insulin', 'BMI']:\n",
        "  db[cl].replace(0, None, inplace=True)\n",
        "  db[cl].fillna(db[cl].median(skipna=True), inplace=True)\n",
        "for cl in ['Glucose','SkinThickness']:\n",
        "  db[cl].replace(0, method='ffill', inplace=True)\n",
        "db"
      ],
      "metadata": {
        "id": "7FOvXgRUTx6C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Check for missing values again."
      ],
      "metadata": {
        "id": "t6hVGYES9iS_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in db.columns:\n",
        "  counts = db[i].value_counts()\n",
        "  if 0 in counts:  # Check if 0 is in the index\n",
        "    print(f\"Column '{i}' has {counts[0]} values at 0.\")"
      ],
      "metadata": {
        "id": "4NO7dzgi9c-x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Practice Problems\n",
        "- Import the dataset titanic.csv\n",
        "- Display it and handle the missing values in the dataset.\n",
        "- Use imputation for numerical data, and forward filling for categorical data.\n",
        "\n",
        "(Hint: Sometimes, it is better to drop a feature than to try and fix it.)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "ibALcneu7uzT"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HnosbSD7-GNO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## b) Outliers\n",
        "- Import the diamonds dataset from seaborn.\n",
        "- Visualise the data as a scatter plot."
      ],
      "metadata": {
        "id": "VCgIdNW7B2UA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Load diamonds dataset\n",
        "diamonds = sns.load_dataset('diamonds')\n",
        "\n",
        "# Scatter plot of carat vs price\n",
        "plt.scatter(diamonds['carat'], diamonds['price'])\n",
        "plt.xlabel('Carat')\n",
        "plt.ylabel('Price')\n",
        "plt.title('Scatter Plot of Carat vs Price')\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "TdDU4vbQIbTM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Identify the outliers in the data.\n",
        "- Remove the outliers using trimming."
      ],
      "metadata": {
        "id": "cjbic9G3Kfou"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove outliers using trimming\n",
        "diamonds_trimmed = diamonds[(diamonds['carat'] < 3.5)]\n",
        "\n",
        "# Scatter plot of carat vs price (trimmed)\n",
        "plt.scatter(diamonds_trimmed['carat'], diamonds_trimmed['price'])\n",
        "plt.xlabel('Carat')\n",
        "plt.ylabel('Price')\n",
        "plt.title('Scatter Plot of Carat vs Price (Trimmed)')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "WKoKBCe9KBLJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Practice Problems:\n",
        "- Identify outliers in the titanic-train.csv dataset using a boxplot on the \"Fare\" feature.\n",
        "- Handle the outliers using trimming.\n"
      ],
      "metadata": {
        "id": "IclJVc89CKtY"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "W8ZwAO1aCzdq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## c) Duplicates"
      ],
      "metadata": {
        "id": "QkUWRpN1E-At"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = {\n",
        "    'Name': ['John', 'Mary', 'John', 'David', 'Mary', 'John'],\n",
        "    'Age': [25, 31, 25, 42, 31, 25],\n",
        "    'City': ['New York', 'Los Angeles', 'New York', 'Chicago', 'Los Angeles', 'New York']\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "print(\"Original Dataset:\")\n",
        "print(df)"
      ],
      "metadata": {
        "id": "Itsnkf58LY-9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Given the sample dataset above, identify the number of duplicate entries.\n"
      ],
      "metadata": {
        "id": "N9Z17MuFKamO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(df.duplicated().sum())"
      ],
      "metadata": {
        "id": "_dRTthNWLk-e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Remove the duplicates."
      ],
      "metadata": {
        "id": "h9rwUIitMBDu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.drop_duplicates(inplace=True)\n",
        "print(\"\\nDataset after removing duplicates:\")\n",
        "print(df)"
      ],
      "metadata": {
        "id": "0cNlkQVsL_ov"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Practice Problems\n",
        "- Identify duplicates in the titanic.csv dataset.\n",
        "- Handle duplicates by dropping them.\n"
      ],
      "metadata": {
        "id": "n3OdlaBuFDRI"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4zcUZDk-FbQo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3) Normalisation\n",
        "Often in machine learning, you will have to deal with datasets that have various sources or data from different types or formats.\n",
        "\n",
        "To prevent any errors in our application, we need to convert all the data into a common format.\n",
        "This process is called normalisation.\n",
        "\n",
        "In this section you will learn about two forms of normalisation."
      ],
      "metadata": {
        "id": "SsyaLN5UMzUw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## a) Encoding\n",
        "Encoding is the process of converting non-numerical data into numerical data that can be understood by the computer.\n"
      ],
      "metadata": {
        "id": "WAFAVSCsNJOf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import OneHotEncoder\n",
        "# Create a sample dataset with categorical variables\n",
        "data = {'color': ['red', 'green', 'blue', 'red', 'blue', 'green'],\n",
        "\n",
        "        'shape': ['circle', 'triangle', 'circle', 'square', 'triangle', 'square']}\n",
        "df = pd.DataFrame(data)\n",
        "print(\"Original dataset:\")\n",
        "df"
      ],
      "metadata": {
        "id": "gHWUKrUkNhJb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Given the above dataset, use one-hot encoding to convert the categorical data (Circle, Square, etc) into numerical data."
      ],
      "metadata": {
        "id": "f7nf39GbONlO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# One-hot encode the categorical variables\n",
        "encoder = OneHotEncoder(sparse_output=False)\n",
        "encoder.fit(df)\n",
        "encoded_df = encoder.transform(df)\n",
        "print(\"One-hot encoded dataset:\")\n",
        "encoded_df"
      ],
      "metadata": {
        "id": "UGdZF_wOOWBA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert the encoded data back to a Pandas DataFrame\n",
        "encoded_df = pd.DataFrame(encoded_df, columns=encoder.get_feature_names_out())\n",
        "encoded_df"
      ],
      "metadata": {
        "id": "tyl7nuykP6-S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Practice Problems:\n",
        "- Using the cleaned titanic.csv dataset, encode the data in the Embarked column."
      ],
      "metadata": {
        "id": "BwsMEf5pQGky"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mWopdkoPQqep"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Use binary encoding to encode the data in the Sex column."
      ],
      "metadata": {
        "id": "YeZOXe3lrBE6"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "iAanKx7UrAzY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## b) Scaling\n",
        "Different sources of data can have different ranges and minimum/maximum values. To ensure that all data is given equal importance, we need to standardize the data.\n",
        "\n",
        "This process is called scaling.\n",
        "\n",
        "We will be using 2 different types of scaling, Standard Scaling (or z-score scaling) and Min-Max Scaling.\n",
        "We will be looking at the effect that scaling has on linear regression (will be explained in detail in the next section)."
      ],
      "metadata": {
        "id": "D_FEVYn_kx6N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "# Load California housing dataset\n",
        "california_housing = fetch_california_housing(as_frame=True)\n",
        "print(california_housing)"
      ],
      "metadata": {
        "id": "GDNntlikwLE5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Given the above dataset about houses in california, identify the target value and feature values.\n",
        "- You can see that the features are numerical with different means and variations.\n",
        "- Use a Standard Scaler and a Mix-Max Scaler and see the effects it has on the error in a Machine Learning Model."
      ],
      "metadata": {
        "id": "MS6qhBwUp-4W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the dataset into features (X) and target (y)\n",
        "chx = california_housing.data\n",
        "chy = california_housing.target\n",
        "# Split the dataset into training and testing sets\n",
        "chx_train, chx_test, chy_train, chy_test = train_test_split(chx, chy, test_size=0.2, random_state=42)\n",
        "print(chx_train)"
      ],
      "metadata": {
        "id": "ylu5OvGBptHW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a StandardScaler object\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# Fit the scaler to the training data and transform both the training and testing data\n",
        "chx_train_scaled = scaler.fit_transform(chx_train)\n",
        "chx_test_scaled = scaler.transform(chx_test)\n",
        "print(chx_train_scaled)"
      ],
      "metadata": {
        "id": "h_q9COWGuExC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a MinMaxScaler object\n",
        "min_max_scaler = MinMaxScaler()\n",
        "\n",
        "# Fit the scaler to the training data and transform both the training and testing data\n",
        "chx_train_min_max_scaled = min_max_scaler.fit_transform(chx_train)\n",
        "chx_test_min_max_scaled = min_max_scaler.transform(chx_test)\n",
        "print(chx_train_min_max_scaled)"
      ],
      "metadata": {
        "id": "M2jb2LeNuFIu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4) Linear Regression\n",
        "Now, we will be working with one of the basic types of machine learning, linear regression. Linear regression is used for predicting a range of values, such as house prices,\n",
        "- Given the differently scaled versions of the california housing dataset, train the model on all 3 and calculate the error in predictions."
      ],
      "metadata": {
        "id": "1Cx4-X5SPgbN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a Linear Regression model\n",
        "model = LinearRegression()"
      ],
      "metadata": {
        "id": "j1THoxTxPvwt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model on the original data\n",
        "model.fit(chx_train, chy_train)\n",
        "chy_pred = model.predict(chx_test)\n",
        "mse_original = mean_squared_error(chy_test, chy_pred)"
      ],
      "metadata": {
        "id": "_rSTlty2P0xC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model on the standard scaled data\n",
        "model.fit(chx_train_scaled, chy_train)\n",
        "chy_pred_scaled = model.predict(chx_test_scaled)\n",
        "mse_scaled = mean_squared_error(chy_test, chy_pred_scaled)"
      ],
      "metadata": {
        "id": "424fGlPdP53f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model on the min-max scaled data\n",
        "model.fit(chx_train_min_max_scaled, chy_train)\n",
        "chy_pred_min_max_scaled = model.predict(chx_test_min_max_scaled)\n",
        "mse_min_max_scaled = mean_squared_error(chy_test, chy_pred_min_max_scaled)"
      ],
      "metadata": {
        "id": "79mQBD8_P8_F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"MSE on original data:\", mse_original)\n",
        "print(\"MSE on scaled data (StandardScaler):\", mse_scaled)\n",
        "print(\"MSE on scaled data (MinMaxScaler):\", mse_min_max_scaled)"
      ],
      "metadata": {
        "id": "pvjYSDInQAAS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Practice Problems\n",
        "- Import the fish_data dataset and view it."
      ],
      "metadata": {
        "id": "TGC17h2R9s3j"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "whpDm54B8eti"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Clean the data as shown in previous sections."
      ],
      "metadata": {
        "id": "FgreM6iX97m0"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "s8UKYTusdz--"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Seperate the dataframe into targets and features"
      ],
      "metadata": {
        "id": "8I-vW8hge_8p"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "1SS2H87rSkVT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Create a 0.2 train-test split and scale the data.\n"
      ],
      "metadata": {
        "id": "o5XJTlNV_cQo"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hg7WgOhl_b9y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Run a linear regression model on the data.\n",
        "- Find the MSE of your model."
      ],
      "metadata": {
        "id": "IiKemy_Aeh83"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xg19oPNaelv1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5) Classification\n",
        "Classification is the second basic use of Machine Learning. It is used when we need to classify data into different groups. (Ex: Medical Diagnosis, image recognition)"
      ],
      "metadata": {
        "id": "quH5etsUfgw0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Import the iris dataset from sklearn.\n",
        "- Scale and clean the data as usual.\n",
        "- Create a 0.3 train, test split and run a classification model using Gaussian Naive-Bayes.\n",
        "- Show the accuracy and confusion matrix of the model."
      ],
      "metadata": {
        "id": "mEErqBPspuUf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_iris\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Load the Iris dataset\n",
        "iris = load_iris()\n",
        "x = iris.data\n",
        "y = iris.target\n",
        "x"
      ],
      "metadata": {
        "id": "t82-f914rOoA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the dataset into training and testing sets\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=42)\n",
        "\n",
        "\n",
        "# Scale the features using StandardScaler\n",
        "scaler = StandardScaler()\n",
        "x_train_scaled = scaler.fit_transform(x_train)\n",
        "x_test_scaled = scaler.transform(x_test)"
      ],
      "metadata": {
        "id": "XYyTar0WsGpK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a Gaussian Naive Bayes classification model\n",
        "gnb = GaussianNB()\n",
        "\n",
        "# Train the model on the scaled training data\n",
        "gnb.fit(x_train_scaled, y_train)\n",
        "\n",
        "# Make predictions on the scaled testing data\n",
        "y_pred = gnb.predict(x_test_scaled)\n",
        "\n",
        "# Evaluate the model's performance\n",
        "accuracy = gnb.score(x_test_scaled, y_test)\n",
        "print(\"Model accuracy:\", accuracy)"
      ],
      "metadata": {
        "id": "RG9Evkq1sI2o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "# Create a confusion matrix\n",
        "cnf_matrix = confusion_matrix(y_test, y_pred)\n",
        "print(\"Confusion matrix:\")\n",
        "print(cnf_matrix)\n"
      ],
      "metadata": {
        "id": "FA7Zy3xhs3gR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "(This is just code to make the confusion matrix look pretty)"
      ],
      "metadata": {
        "id": "9tF9yt9gvARu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import itertools\n",
        "def plot_confusion_matrix(cm, classes, title='Confusion matrix', cmap=plt.cm.Blues):\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    tick_marks = np.arange(len(classes))\n",
        "    plt.xticks(tick_marks, classes, rotation=45)\n",
        "    plt.yticks(tick_marks, classes)\n",
        "    cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
        "    print(\"Normalized confusion matrix\")\n",
        "    print(cm)\n",
        "\n",
        "    thresh = cm.max() / 2.\n",
        "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
        "        plt.text(j, i, round(cm[i, j],3),\n",
        "                 horizontalalignment=\"center\",\n",
        "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')\n",
        "class_names = iris.target_names\n",
        "plot_confusion_matrix(cnf_matrix, classes=class_names,\n",
        "                      title='Confusion matrix, with normalization')"
      ],
      "metadata": {
        "id": "ZbC8kTMCvDX1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Practice Problems\n",
        "- Using the titanic dataset that you have been working on until now, identify the target and features.\n",
        "\n",
        "(Hint: Think about which parameters are relevant to the model, and which parameters can be ignored and dropped.)"
      ],
      "metadata": {
        "id": "cEMrmyLgtiLP"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "h6VJWunCyjon"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Create a 0.2 train-test split and run a GaussianNB model.\n"
      ],
      "metadata": {
        "id": "6KI6WhKv4vcM"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gOcW_XAP4xAQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Check the accuracy of the model, and create a confusion matrix.\n"
      ],
      "metadata": {
        "id": "1NFP540n4xPH"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MJC-4spM4yMR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Bonus: Create a correlation matrix between the features.\n"
      ],
      "metadata": {
        "id": "t3rUMycN4ykq"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "gGmQsvRL4y_A"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}